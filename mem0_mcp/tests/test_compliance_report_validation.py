"""
Test Suite for MCP Compliance Report Generation and Validation

Tests the accuracy and quality of compliance reports generated by the MCP
compliance analysis system, including JSON output, remediation recommendations,
and risk scoring accuracy.
"""

import pytest
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import tempfile
import shutil

@dataclass
class ComplianceReport:
    """Structure for compliance analysis report"""
    analysis_summary: Dict[str, Any]
    violations_by_severity: Dict[str, int]
    violations: List[Dict[str, Any]]
    recommendations: List[Dict[str, Any]]
    file_analysis: Dict[str, Any]
    timestamp: str
    mcp_version: str

@dataclass
class ViolationValidationResult:
    """Result of validating a specific violation"""
    violation_id: str
    is_valid: bool
    line_number_accurate: bool
    severity_appropriate: bool
    recommendation_actionable: bool
    code_example_provided: bool
    validation_details: Dict[str, Any]

class ComplianceReportValidator:
    """Validates compliance reports for accuracy and completeness"""
    
    def __init__(self):
        self.required_report_fields = [
            "analysis_summary", "violations_by_severity", "violations", 
            "recommendations", "file_analysis", "timestamp", "mcp_version"
        ]
        self.required_violation_fields = [
            "id", "type", "severity", "risk_score", "file_path", 
            "line", "description", "recommendation"
        ]
        self.severity_levels = ["critical", "high", "medium", "low"]
        self.risk_score_ranges = {"critical": (8, 10), "high": (6, 7), "medium": (4, 5), "low": (1, 3)}
    
    def validate_report_structure(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """Validate the structure of a compliance report"""
        validation_result = {
            "structure_valid": True,
            "missing_fields": [],
            "field_type_errors": [],
            "summary_validation": {},
            "details": []
        }
        
        # Check required top-level fields
        for field in self.required_report_fields:
            if field not in report:
                validation_result["missing_fields"].append(field)
                validation_result["structure_valid"] = False
        
        # Validate field types
        expected_types = {
            "analysis_summary": dict,
            "violations_by_severity": dict,
            "violations": list,
            "recommendations": list,
            "file_analysis": dict,
            "timestamp": str,
            "mcp_version": str
        }
        
        for field, expected_type in expected_types.items():
            if field in report and not isinstance(report[field], expected_type):
                validation_result["field_type_errors"].append({
                    "field": field,
                    "expected_type": expected_type.__name__,
                    "actual_type": type(report[field]).__name__
                })
                validation_result["structure_valid"] = False
        
        # Validate analysis summary
        if "analysis_summary" in report:
            summary_validation = self._validate_analysis_summary(report["analysis_summary"])
            validation_result["summary_validation"] = summary_validation
            if not summary_validation["valid"]:
                validation_result["structure_valid"] = False
        
        return validation_result
    
    def validate_violations(self, violations: List[Dict[str, Any]]) -> List[ViolationValidationResult]:
        """Validate individual violations in the report"""
        validation_results = []
        
        for violation in violations:
            result = self._validate_single_violation(violation)
            validation_results.append(result)
        
        return validation_results
    
    def validate_risk_scoring(self, violations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Validate risk scoring consistency and accuracy"""
        scoring_validation = {
            "consistent_scoring": True,
            "score_range_violations": [],
            "severity_score_mismatches": [],
            "score_distribution": {},
            "average_scores_by_severity": {}
        }
        
        # Group violations by severity
        by_severity = {}
        for violation in violations:
            severity = violation.get("severity", "unknown")
            if severity not in by_severity:
                by_severity[severity] = []
            by_severity[severity].append(violation.get("risk_score", 0))
        
        # Validate score ranges for each severity
        for severity, scores in by_severity.items():
            if severity in self.risk_score_ranges:
                min_score, max_score = self.risk_score_ranges[severity]
                
                for score in scores:
                    if not (min_score <= score <= max_score):
                        scoring_validation["score_range_violations"].append({
                            "severity": severity,
                            "score": score,
                            "expected_range": (min_score, max_score)
                        })
                        scoring_validation["consistent_scoring"] = False
                
                # Calculate average score for severity
                avg_score = sum(scores) / len(scores) if scores else 0
                scoring_validation["average_scores_by_severity"][severity] = avg_score
                
                # Check if average score is in expected range
                if not (min_score <= avg_score <= max_score):
                    scoring_validation["severity_score_mismatches"].append({
                        "severity": severity,
                        "average_score": avg_score,
                        "expected_range": (min_score, max_score)
                    })
        
        # Score distribution
        scoring_validation["score_distribution"] = {
            str(score): sum(1 for v in violations if v.get("risk_score") == score)
            for score in range(1, 11)
        }
        
        return scoring_validation
    
    def validate_remediation_quality(self, violations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Validate quality of remediation recommendations"""
        remediation_validation = {
            "all_have_recommendations": True,
            "recommendations_actionable": True,
            "code_examples_provided": 0,
            "recommendation_quality_scores": [],
            "missing_recommendations": [],
            "weak_recommendations": []
        }
        
        for violation in violations:
            violation_id = violation.get("id", "unknown")
            recommendation = violation.get("recommendation", "")
            
            if not recommendation:
                remediation_validation["all_have_recommendations"] = False
                remediation_validation["missing_recommendations"].append(violation_id)
                continue
            
            # Assess recommendation quality
            quality_score = self._assess_recommendation_quality(recommendation, violation)
            remediation_validation["recommendation_quality_scores"].append({
                "violation_id": violation_id,
                "score": quality_score
            })
            
            if quality_score < 3:  # Quality score out of 5
                remediation_validation["weak_recommendations"].append({
                    "violation_id": violation_id,
                    "recommendation": recommendation,
                    "score": quality_score
                })
                
            # Check for code examples
            if "code" in recommendation.lower() or "example" in recommendation.lower():
                remediation_validation["code_examples_provided"] += 1
        
        # Calculate overall actionability
        avg_quality = sum(r["score"] for r in remediation_validation["recommendation_quality_scores"]) / len(remediation_validation["recommendation_quality_scores"]) if remediation_validation["recommendation_quality_scores"] else 0
        remediation_validation["recommendations_actionable"] = avg_quality >= 3.5
        
        return remediation_validation
    
    def validate_line_number_accuracy(self, violations: List[Dict[str, Any]], source_files: Dict[str, str]) -> Dict[str, Any]:
        """Validate accuracy of line number references in violations"""
        line_validation = {
            "accurate_line_numbers": 0,
            "inaccurate_line_numbers": 0,
            "line_accuracy_percentage": 0.0,
            "accuracy_details": []
        }
        
        for violation in violations:
            file_path = violation.get("file_path", "")
            line_number = violation.get("line", 0)
            violation_type = violation.get("type", "")
            
            # Check if file exists in source files
            if file_path in source_files:
                file_content = source_files[file_path]
                lines = file_content.split('\n')
                
                if 1 <= line_number <= len(lines):
                    actual_line = lines[line_number - 1]
                    is_accurate = self._check_line_relevance(actual_line, violation_type, violation.get("description", ""))
                    
                    if is_accurate:
                        line_validation["accurate_line_numbers"] += 1
                    else:
                        line_validation["inaccurate_line_numbers"] += 1
                    
                    line_validation["accuracy_details"].append({
                        "violation_id": violation.get("id", "unknown"),
                        "file_path": file_path,
                        "line_number": line_number,
                        "actual_line": actual_line.strip(),
                        "is_accurate": is_accurate,
                        "violation_type": violation_type
                    })
                else:
                    line_validation["inaccurate_line_numbers"] += 1
                    line_validation["accuracy_details"].append({
                        "violation_id": violation.get("id", "unknown"),
                        "file_path": file_path,
                        "line_number": line_number,
                        "actual_line": "LINE_OUT_OF_RANGE",
                        "is_accurate": False,
                        "error": "Line number exceeds file length"
                    })
        
        # Calculate accuracy percentage
        total_violations = line_validation["accurate_line_numbers"] + line_validation["inaccurate_line_numbers"]
        if total_violations > 0:
            line_validation["line_accuracy_percentage"] = (line_validation["accurate_line_numbers"] / total_violations) * 100
        
        return line_validation
    
    def _validate_analysis_summary(self, summary: Dict[str, Any]) -> Dict[str, Any]:
        """Validate the analysis summary section"""
        required_summary_fields = [
            "total_files_analyzed", "total_violations", "overall_compliance_score", 
            "timestamp", "mcp_version"
        ]
        
        validation = {"valid": True, "missing_fields": [], "type_errors": []}
        
        for field in required_summary_fields:
            if field not in summary:
                validation["missing_fields"].append(field)
                validation["valid"] = False
        
        # Validate specific field types and ranges
        if "total_files_analyzed" in summary:
            if not isinstance(summary["total_files_analyzed"], int) or summary["total_files_analyzed"] < 0:
                validation["type_errors"].append("total_files_analyzed must be non-negative integer")
                validation["valid"] = False
        
        if "overall_compliance_score" in summary:
            score = summary["overall_compliance_score"]
            if not isinstance(score, (int, float)) or not (0.0 <= score <= 10.0):
                validation["type_errors"].append("overall_compliance_score must be float between 0.0 and 10.0")
                validation["valid"] = False
        
        if "mcp_version" in summary:
            if summary["mcp_version"] != "2025-06-18":
                validation["type_errors"].append(f"mcp_version should be '2025-06-18', got '{summary['mcp_version']}'")
                validation["valid"] = False
        
        return validation
    
    def _validate_single_violation(self, violation: Dict[str, Any]) -> ViolationValidationResult:
        """Validate a single violation entry"""
        violation_id = violation.get("id", "unknown")
        
        # Check required fields
        has_required_fields = all(field in violation for field in self.required_violation_fields)
        
        # Check line number validity
        line_number = violation.get("line", 0)
        line_number_valid = isinstance(line_number, int) and line_number > 0
        
        # Check severity validity
        severity = violation.get("severity", "")
        severity_valid = severity in self.severity_levels
        
        # Check risk score consistency
        risk_score = violation.get("risk_score", 0)
        score_consistent = True
        if severity_valid and severity in self.risk_score_ranges:
            min_score, max_score = self.risk_score_ranges[severity]
            score_consistent = min_score <= risk_score <= max_score
        
        # Check recommendation quality
        recommendation = violation.get("recommendation", "")
        recommendation_quality = self._assess_recommendation_quality(recommendation, violation)
        
        return ViolationValidationResult(
            violation_id=violation_id,
            is_valid=has_required_fields,
            line_number_accurate=line_number_valid,
            severity_appropriate=severity_valid,
            recommendation_actionable=recommendation_quality >= 3,
            code_example_provided="example" in recommendation.lower() or "code" in recommendation.lower(),
            validation_details={
                "has_required_fields": has_required_fields,
                "missing_fields": [f for f in self.required_violation_fields if f not in violation],
                "severity_valid": severity_valid,
                "risk_score_consistent": score_consistent,
                "recommendation_quality": recommendation_quality
            }
        )
    
    def _assess_recommendation_quality(self, recommendation: str, violation: Dict[str, Any]) -> int:
        """Assess quality of a remediation recommendation (1-5 scale)"""
        if not recommendation:
            return 0
        
        quality_score = 1  # Base score
        
        # Length and detail (up to +1)
        if len(recommendation) > 50:
            quality_score += 1
        
        # Specificity (up to +1)
        specific_keywords = ["change", "add", "remove", "update", "implement", "fix"]
        if any(keyword in recommendation.lower() for keyword in specific_keywords):
            quality_score += 1
        
        # Code examples (up to +1)
        if "example" in recommendation.lower() or "code" in recommendation.lower():
            quality_score += 1
        
        # References to MCP specification (up to +1)
        mcp_references = ["mcp", "json-rpc", "protocol", "specification"]
        if any(ref in recommendation.lower() for ref in mcp_references):
            quality_score += 1
        
        return min(quality_score, 5)
    
    def _check_line_relevance(self, line_content: str, violation_type: str, description: str) -> bool:
        """Check if violation line number is relevant to the violation type"""
        line_lower = line_content.lower()
        
        # Define relevance keywords for different violation types
        relevance_patterns = {
            "protocol_violation": ["jsonrpc", "protocol", "version", "2.0"],
            "transport_violation": ["http", "transport", "session", "header", "sse"],
            "security_violation": ["origin", "cors", "security", "validation", "auth"],
            "tool_violation": ["tool", "schema", "input", "output", "method"],
            "flow_violation": ["initialize", "sequence", "flow", "request", "response"]
        }
        
        patterns = relevance_patterns.get(violation_type, [])
        
        # Check if line contains relevant keywords
        return any(pattern in line_lower for pattern in patterns) or any(pattern in description.lower() for pattern in patterns)

class TestComplianceReportGeneration:
    """Test suite for compliance report generation and validation"""
    
    def setup_method(self):
        """Setup for each test method"""
        self.validator = ComplianceReportValidator()
        self.test_data_dir = Path("/tmp/mcp_compliance_test")
        self.test_data_dir.mkdir(exist_ok=True)
    
    def teardown_method(self):
        """Cleanup after each test method"""
        if self.test_data_dir.exists():
            shutil.rmtree(self.test_data_dir)
    
    def test_report_structure_validation(self):
        """Test validation of report structure"""
        # Valid report structure
        valid_report = {
            "analysis_summary": {
                "total_files_analyzed": 5,
                "total_violations": 10,
                "overall_compliance_score": 7.5,
                "timestamp": "2025-01-XX",
                "mcp_version": "2025-06-18"
            },
            "violations_by_severity": {"critical": 2, "high": 3, "medium": 4, "low": 1},
            "violations": [],
            "recommendations": [],
            "file_analysis": {},
            "timestamp": "2025-01-XX",
            "mcp_version": "2025-06-18"
        }
        
        validation_result = self.validator.validate_report_structure(valid_report)
        assert validation_result["structure_valid"], f"Valid report failed validation: {validation_result}"
        assert len(validation_result["missing_fields"]) == 0, "No fields should be missing"
        assert len(validation_result["field_type_errors"]) == 0, "No type errors expected"
    
    def test_invalid_report_structure_detection(self):
        """Test detection of invalid report structures"""
        # Missing required fields
        invalid_report = {
            "analysis_summary": {},
            "violations": []
            # Missing other required fields
        }
        
        validation_result = self.validator.validate_report_structure(invalid_report)
        assert not validation_result["structure_valid"], "Invalid report should fail validation"
        assert len(validation_result["missing_fields"]) > 0, "Should detect missing fields"
    
    def test_violation_structure_validation(self):
        """Test validation of individual violations"""
        valid_violations = [
            {
                "id": "V001",
                "type": "protocol_violation",
                "severity": "critical",
                "risk_score": 9,
                "file_path": "/test/file.py",
                "line": 42,
                "description": "Incorrect JSON-RPC version",
                "recommendation": "Change jsonrpc version to 2.0"
            }
        ]
        
        validation_results = self.validator.validate_violations(valid_violations)
        assert len(validation_results) == 1
        assert validation_results[0].is_valid, "Valid violation should pass validation"
        assert validation_results[0].severity_appropriate, "Severity should be appropriate"
        assert validation_results[0].line_number_accurate, "Line number should be valid"
    
    def test_risk_scoring_validation(self):
        """Test validation of risk scoring consistency"""
        violations_with_scores = [
            {"severity": "critical", "risk_score": 9},
            {"severity": "critical", "risk_score": 8},
            {"severity": "high", "risk_score": 7},
            {"severity": "high", "risk_score": 6},
            {"severity": "medium", "risk_score": 5},
            {"severity": "low", "risk_score": 2},
            {"severity": "critical", "risk_score": 5},  # Inconsistent score
        ]
        
        scoring_validation = self.validator.validate_risk_scoring(violations_with_scores)
        
        assert not scoring_validation["consistent_scoring"], "Should detect inconsistent scoring"
        assert len(scoring_validation["score_range_violations"]) == 1, "Should detect one score range violation"
        assert scoring_validation["score_range_violations"][0]["severity"] == "critical"
        assert scoring_validation["score_range_violations"][0]["score"] == 5
    
    def test_remediation_quality_assessment(self):
        """Test assessment of remediation recommendation quality"""
        violations_with_recommendations = [
            {
                "id": "V001",
                "recommendation": "Change the version"  # Weak recommendation
            },
            {
                "id": "V002", 
                "recommendation": "Update the JSON-RPC version from '1.0' to '2.0' in the response object. Example: {'jsonrpc': '2.0', 'result': data}"  # Good recommendation
            },
            {
                "id": "V003",
                "recommendation": ""  # Missing recommendation
            }
        ]
        
        quality_validation = self.validator.validate_remediation_quality(violations_with_recommendations)
        
        assert not quality_validation["all_have_recommendations"], "Should detect missing recommendations"
        assert len(quality_validation["missing_recommendations"]) == 1, "Should find one missing recommendation"
        assert quality_validation["code_examples_provided"] >= 1, "Should detect code examples"
        
        # Check quality scores
        quality_scores = {r["violation_id"]: r["score"] for r in quality_validation["recommendation_quality_scores"]}
        assert quality_scores["V001"] < quality_scores["V002"], "Better recommendation should have higher score"
    
    def test_line_number_accuracy_validation(self):
        """Test validation of line number accuracy"""
        # Create test source files
        source_files = {
            "/test/file.py": '''def test_function():
    return {
        "jsonrpc": "1.0",  # Line 3 - violation here
        "result": "data"
    }'''
        }
        
        violations = [
            {
                "id": "V001",
                "file_path": "/test/file.py",
                "line": 3,
                "type": "protocol_violation",
                "description": "Incorrect JSON-RPC version"
            },
            {
                "id": "V002",
                "file_path": "/test/file.py", 
                "line": 10,  # Line doesn't exist
                "type": "protocol_violation",
                "description": "Another violation"
            }
        ]
        
        line_validation = self.validator.validate_line_number_accuracy(violations, source_files)
        
        assert line_validation["accurate_line_numbers"] == 1, "Should find one accurate line number"
        assert line_validation["inaccurate_line_numbers"] == 1, "Should find one inaccurate line number"
        assert line_validation["line_accuracy_percentage"] == 50.0, "Should calculate 50% accuracy"
    
    def test_comprehensive_report_validation(self):
        """Test comprehensive validation of a complete report"""
        # Create a comprehensive test report
        test_report = {
            "analysis_summary": {
                "total_files_analyzed": 3,
                "total_violations": 5,
                "overall_compliance_score": 6.8,
                "timestamp": "2025-01-XX",
                "mcp_version": "2025-06-18",
                "analysis_duration": 2.5
            },
            "violations_by_severity": {
                "critical": 1,
                "high": 2, 
                "medium": 1,
                "low": 1
            },
            "violations": [
                {
                    "id": "V001",
                    "type": "protocol_violation",
                    "severity": "critical",
                    "risk_score": 9,
                    "file_path": "/test/transport.py",
                    "line": 15,
                    "description": "Using JSON-RPC 1.0 instead of 2.0",
                    "recommendation": "Update JSON-RPC version to '2.0' in all response objects. Example: {'jsonrpc': '2.0', 'result': result, 'id': request_id}"
                },
                {
                    "id": "V002",
                    "type": "security_violation",
                    "severity": "high",
                    "risk_score": 7,
                    "file_path": "/test/server.py",
                    "line": 32,
                    "description": "Missing origin validation",
                    "recommendation": "Implement origin header validation to prevent CSRF attacks"
                }
            ],
            "recommendations": [
                {
                    "priority": "critical",
                    "category": "protocol_compliance",
                    "description": "Fix JSON-RPC version issues",
                    "affected_files": ["/test/transport.py"],
                    "effort_estimate": "low"
                }
            ],
            "file_analysis": {
                "/test/transport.py": {
                    "compliance_score": 6.5,
                    "violations_count": 3,
                    "lines_analyzed": 120
                }
            },
            "timestamp": "2025-01-XX",
            "mcp_version": "2025-06-18"
        }
        
        # Validate structure
        structure_validation = self.validator.validate_report_structure(test_report)
        assert structure_validation["structure_valid"], "Report structure should be valid"
        
        # Validate violations
        violation_validations = self.validator.validate_violations(test_report["violations"])
        for validation in violation_validations:
            assert validation.is_valid, f"Violation {validation.violation_id} should be valid"
        
        # Validate scoring
        scoring_validation = self.validator.validate_risk_scoring(test_report["violations"])
        assert scoring_validation["consistent_scoring"], "Risk scoring should be consistent"
        
        # Validate remediation quality
        remediation_validation = self.validator.validate_remediation_quality(test_report["violations"])
        assert remediation_validation["all_have_recommendations"], "All violations should have recommendations"
    
    def test_json_output_format_compliance(self):
        """Test JSON serialization and format compliance"""
        test_report = {
            "analysis_summary": {"total_files_analyzed": 1},
            "violations_by_severity": {"critical": 0},
            "violations": [],
            "recommendations": [],
            "file_analysis": {},
            "timestamp": "2025-01-XX",
            "mcp_version": "2025-06-18"
        }
        
        # Test JSON serialization
        try:
            json_output = json.dumps(test_report, indent=2)
            assert isinstance(json_output, str), "Should serialize to JSON string"
            
            # Test deserialization
            parsed_report = json.loads(json_output)
            assert parsed_report == test_report, "Should deserialize to original structure"
            
            # Test pretty printing
            pretty_output = json.dumps(test_report, indent=2, sort_keys=True)
            assert len(pretty_output.split('\n')) > 5, "Should produce multi-line pretty output"
            
        except (TypeError, ValueError) as e:
            pytest.fail(f"JSON serialization failed: {e}")
    
    def test_performance_report_generation(self):
        """Test performance of report generation with large datasets"""
        # Create large test dataset
        large_violations = []
        for i in range(1000):
            violation = {
                "id": f"V{i:04d}",
                "type": "protocol_violation",
                "severity": ["critical", "high", "medium", "low"][i % 4],
                "risk_score": (i % 10) + 1,
                "file_path": f"/test/file_{i % 10}.py",
                "line": i + 1,
                "description": f"Violation description {i}",
                "recommendation": f"Fix recommendation {i}"
            }
            large_violations.append(violation)
        
        # Test validation performance
        start_time = time.time()
        validation_results = self.validator.validate_violations(large_violations)
        validation_time = time.time() - start_time
        
        assert len(validation_results) == 1000, "Should validate all violations"
        assert validation_time < 5.0, f"Validation took too long: {validation_time}s"
        
        # Test scoring validation performance
        start_time = time.time()
        scoring_validation = self.validator.validate_risk_scoring(large_violations)
        scoring_time = time.time() - start_time
        
        assert scoring_time < 2.0, f"Scoring validation took too long: {scoring_time}s"

if __name__ == "__main__":
    # Run report validation tests when executed directly
    validator = ComplianceReportValidator()
    
    print("MCP Compliance Report Validation Tests")
    print("=" * 45)
    
    # Test basic validation capabilities
    test_report = {
        "analysis_summary": {
            "total_files_analyzed": 2,
            "total_violations": 3,
            "overall_compliance_score": 8.2,
            "timestamp": "2025-01-XX",
            "mcp_version": "2025-06-18"
        },
        "violations_by_severity": {"critical": 1, "high": 1, "medium": 1},
        "violations": [
            {
                "id": "V001",
                "type": "protocol_violation",
                "severity": "critical",
                "risk_score": 9,
                "file_path": "/test.py",
                "line": 5,
                "description": "Test violation",
                "recommendation": "Fix this issue"
            }
        ],
        "recommendations": [],
        "file_analysis": {},
        "timestamp": "2025-01-XX",
        "mcp_version": "2025-06-18"
    }
    
    structure_validation = validator.validate_report_structure(test_report)
    print(f"Structure validation: {'PASS' if structure_validation['structure_valid'] else 'FAIL'}")
    
    if test_report["violations"]:
        violation_validations = validator.validate_violations(test_report["violations"])
        print(f"Violation validations: {len([v for v in violation_validations if v.is_valid])}/{len(violation_validations)} valid")
    
    # Run with pytest
    import sys
    exit_code = pytest.main([__file__, "-v", "--tb=short"])
    sys.exit(exit_code)